---
title: "Final Project"
author: "David Counter"
date: "December 17, 2019"
output: html_document
---

#Introduction

	The Stat 425 final project is about working with big data and creating the best model to use for the predication of sales of products as it relates to weather events. This data was aggregated by Walmart back in 2014, taken from 45 different stores with 20 different weather reporting stations across these areas. The train data provided by Walmart gives us the data across these stores for the amount of a particular item sold, its item id and is organized by date. The test csv is what Kaggle tests our model against; the data for these dates and stores is known, and we will be given a score based upon how accurate our model is in comparison to its actual values.

	Walmart also of course provides weather data across 20 stations per day. There’s extensive data on rainfall, wind speed, temperature, significant weather events and so on. Predictors like pressure, monthly rainfall, average sea level pressure and so on might not be that influential in terms of the final model, however we will see should our prediction on these variables might change. We will have to use these potential 74 variables to create a model which predicts the sales of each product in the test data. First initially starting with a “full” Ordinary least squares model, we will do some model selection, possible ridge or lasso scaling, and take into account collinearity, covariance and so on.  After this model selection, we will use it to predict the data with the test.csv data. Kaggle will return a score, the lower of which will be better. Our lowest Kaggle score will be our “best model” and this is what will be scored in the final grade. Finally, at the end we’ll include all relevant graphs and tables, as well as all code that is used in the report.
	
	
#Linear Regression Model / Diagnostics 
```{r data load}
#loaded Packages
library(foreach)
library(parallel)
library(doParallel)
library(dplyr)

#initialized data
train <- read.csv("train.csv")
weather <- read.csv("weather.csv")
test<- read.csv("test.csv")


#set backup
trainbackup <- train
weatherbackup <- weather
testbackup <- test
colnames(weather)[1] <- ("store_nbr")


#set dates to date format
train$date <- as.Date(train$date, "%Y-%m-%d")
weather$date <- as.Date(weather$date, "%Y-%m-%d")
test$date <- as.Date(test$date, "%Y-%m-%d")



```

```{r data merge}

keymerge <- rep(1,4617600)

key<- read.csv("key.csv")

#Don't run it this way unless you have 24 hours to let your computer run
#converts all store numbers based on key to merge with weather
  mcmapply( FUN = for(i in 1:4617600){
    keymerge[i] = key$station_nbr[train$store_nbr[i]]
    if(i %% 10000 == 0){
      print(i)
    }
  }
)

train<-cbind("station_nbr" = keymerge, train)
  
mergedata <- merge(train, weather, by = c("date", "station_nbr"))
mergedata <- mergedata[with(mergedata, order(mergedata$date, mergedata$store_nbr, mergedata$item_nbr)),]
write.csv(mergedata, "mergedata.csv")

```

```{r dataclean}
#applies data converting to remove as factor variables
mergedata <- read.csv("mergedata.csv")

mergedata$date <- as.Date(mergedata$date, "%Y-%m-%d")
mergedata$tmax <- as.numeric(levels(mergedata$tmax))[mergedata$tmax]
mergedata$tmin <- as.numeric(levels(mergedata$tmin))[mergedata$tmin]
mergedata$tavg <- as.numeric(levels(mergedata$tavg))[mergedata$tavg]
mergedata$depart <- as.numeric(levels(mergedata$depart))[mergedata$depart]
mergedata$dewpoint <- as.numeric(levels(mergedata$dewpoint))[mergedata$dewpoint]
mergedata$wetbulb <- as.numeric(levels(mergedata$wetbulb))[mergedata$wetbulb]
mergedata$heat <- as.numeric(levels(mergedata$heat))[mergedata$heat]
mergedata$cool <- as.numeric(levels(mergedata$cool))[mergedata$cool]
mergedata$sunrise <- as.numeric(levels(mergedata$sunrise))[mergedata$sunrise]
mergedata$sunset <- as.numeric(levels(mergedata$sunset))[mergedata$sunset]
mergedata$snowfall <- as.numeric(levels(mergedata$snowfall))[mergedata$snowfall]
mergedata$preciptotal <- as.numeric(levels(mergedata$preciptotal))[mergedata$preciptotal]
mergedata$stnpressure <- as.numeric(levels(mergedata$stnpressure))[mergedata$stnpressure]
mergedata$sealevel <- as.numeric(levels(mergedata$sealevel))[mergedata$sealevel]
mergedata$resultspeed <- as.numeric(levels(mergedata$resultspeed))[mergedata$resultspeed]
mergedata$resultdir <- as.numeric(levels(mergedata$resultdir))[mergedata$resultdir]
mergedata$avgspeed <- as.numeric(levels(mergedata$avgspeed))[mergedata$avgspeed]


#drop codesum
drops <- c("X", "codesum")
mergedata <- mergedata[ , !names(mergedata) %in% drops]

#clean data, removing any points which contain 0 in units
cleandata <- subset(mergedata, mergedata$units != 0)
drops <- c("X.1", "X")
cleandata <- cleandata[ , !names(cleandata) %in% drops]
write.csv(cleandata, "cleandata.csv")


#reads and drops clean sum, converts date to date format
cleandata <- read.csv("cleandata.csv")
drops <- c("codesum", "X")
cleandata <- cleandata[ , !names(cleandata) %in% drops]
cleandata$date <- as.Date(cleandata$date, "%Y-%m-%d")


#merges test data with merge data to create a from for prediction
newtest <- merge(test, mergedata,  all.x = TRUE)
drops <- c("X.1", "X")
newtest <- newtest[ , !names(newtest) %in% drops]

#deletes all duplicate rows and matches all data points for test without repitition
newtest <- distinct(newtest, .keep_all = TRUE)
newtest <- newtest[!duplicated(newtest[,1:3]),]
```

```{r OLS}
sample <- read.csv("sampleSubmission.csv")
SampleOLS <- sample

#OLS model
OLSModel <- lm(units ~ . , data = cleandata)

summary(OLSModel)


predictionOLS <- predict.lm(OLSModel, newdata = newtest)
predictionOLS[is.na(predictionOLS)] <- 0

SampleOLS$units <- predictionOLS

write.csv(x = SampleOLS, file =  "SampleOLS.csv", row.names = FALSE)


```






























